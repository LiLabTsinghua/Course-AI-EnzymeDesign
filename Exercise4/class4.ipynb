{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272972c7",
   "metadata": {},
   "source": [
    "## This code cell is used to import the modules and libraries required for the project, including data processing, deep learning model training, machine learning algorithm application, and bioinformatics-related tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data processing and analysis\n",
    "import pandas as pd  # For handling tabular data\n",
    "import numpy as np  # Provides efficient array operations and mathematical tools\n",
    "import json  # For loading and saving data in JSON format\n",
    "import re  # Provides regular expression functions for string matching and replacement\n",
    "\n",
    "# Import progress bar tool\n",
    "from tqdm import tqdm  # For displaying a progress bar during loops, improving code interactivity\n",
    "\n",
    "# Import PyTorch and related tools\n",
    "import torch  # Deep learning framework for building and training neural networks\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split  # Tools for data loading and processing\n",
    "import torch.nn as nn  # Provides neural network modules\n",
    "import torch.optim as optim  # Provides optimization algorithms\n",
    "import esm  # Toolkit for protein sequence modeling\n",
    "\n",
    "# Import machine learning related tools\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and test sets\n",
    "from sklearn.ensemble import ExtraTreesClassifier  # Provides an extended implementation of the random forest algorithm\n",
    "from sklearn.preprocessing import LabelEncoder  # For converting labels into numeric encoding\n",
    "\n",
    "# Import general tools\n",
    "from functools import partial  # For creating functions with partially applied arguments\n",
    "import multiprocessing  # Provides parallel computing functionality\n",
    "import pickle  # For saving and loading binary objects\n",
    "\n",
    "# Import bioinformatics tools\n",
    "from Bio import pairwise2  # Provides sequence alignment functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cdbbe73b678058",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Enzyme Functional Annotation\n",
    "### Predicting EC Numbers Based on Protein Sequences\n",
    "### Today's exercise focuses on training and analyzing an EC number prediction model, addressing the following aspects:\n",
    "\n",
    "- 1. Data Preprocessing: Reviewing EC numbers and generating protein/enzyme embeddings.\n",
    "- 2. Machine Learning Model Training and Analysis: Developing a machine learning model for enzyme EC number prediction.\n",
    "- 3. Deep Learning Model Training and Analysis: Constructing a deep learning model for enzyme EC number prediction.\n",
    "- 4. Prediction Using Published Models: Utilizing published deep learning models (CLEAN and DeepECTransformer) to predict EC numbers for E. coli sequences.\n",
    "- 5. Result Analysis and Comparison: Analyzing and comparing the prediction results from CLEAN and DeepECTransformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de3511ef988b09",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102f841",
   "metadata": {},
   "source": [
    "## 1.1 Introduction to EC Numbers\n",
    "- EC Number (Enzyme Commission Number) is a standardized naming system used for classifying and describing enzyme-catalyzed reactions. It consists of four - - - groups of numbers, each representing different aspects of the enzyme: its class, subclass, sub-subclass, and specific reaction.\n",
    "- EC 1.1.1.1 represents alcohol dehydrogenase, with the following specific structure:\n",
    "- 1: Oxidoreductases\n",
    "- 1.1: Oxidoreductases acting on alcohol groups\n",
    "- 1.1.1: Acting on NAD⁺ or NADP⁺ as electron acceptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd408f86",
   "metadata": {},
   "source": [
    "### This code cell is used to load and clean the UniProt dataset, processing and encoding the EC numbers of enzymes. The primary goal of the cleaned data is to provide high-quality training samples for subsequent models, while also preserving the EC number label mapping for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e6de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to validate EC numbers\n",
    "def is_valid_ec_number(ec_number):\n",
    "    \"\"\"\n",
    "    Validates if an EC number is in the correct format.\n",
    "    Uses a regular expression to match a valid 4-part EC number:\n",
    "    1. Composed of four groups of digits separated by '.'.\n",
    "    2. For example: '1.1.1.1' is valid, but '1.1.1' or '1.1.1.a' are not valid.\n",
    "    \n",
    "    Parameters:\n",
    "    - ec_number (str): The EC number to validate.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if valid, False if invalid.\n",
    "    \"\"\"\n",
    "    pattern = r\"^\\d+\\.\\d+\\.\\d+\\.\\d+$\"  # Regular expression for matching EC number\n",
    "    return bool(re.match(pattern, ec_number))\n",
    "\n",
    "# Example usage\n",
    "examples = [\"1.1.1.1\", \"1.1.1\", \"1.1.1.a\", \"2.7.1.12\"]\n",
    "example_results = [is_valid_ec_number(ec) for ec in examples]\n",
    "print(f\"Validation results: {dict(zip(examples, example_results))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f088f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UniProt data\n",
    "uniprot_data_file = './uniprotkb_organism_id_9606_2024_11_21.tsv'  # Path to the UniProt data file\n",
    "uniprot_data = pd.read_csv(uniprot_data_file, sep='\\t')  # Read the TSV file into a DataFrame\n",
    "\n",
    "# Data cleaning\n",
    "# Filter out records where EC number or sequence is missing\n",
    "uniprot_data = uniprot_data[uniprot_data['EC number'].notna() & uniprot_data['Sequence'].notna()]\n",
    "\n",
    "# Clean EC numbers, keeping only valid ones\n",
    "uniprot_data['EC number'] = uniprot_data['EC number'].apply(\n",
    "    lambda x: [ec for ec in x.split('; ') if is_valid_ec_number(ec)]\n",
    ")\n",
    "\n",
    "# Filter out records with sequence length greater than 1000 to avoid interference with model training\n",
    "uniprot_data = uniprot_data[uniprot_data['Sequence'].apply(lambda x: len(x) < 1000)]\n",
    "\n",
    "# Expand multiple EC numbers into separate rows, remove null values, and reset index\n",
    "uniprot_data = uniprot_data.explode('EC number').dropna(subset=['EC number']).reset_index(drop=True)\n",
    "\n",
    "# For convenience, retain only the first 2000 records\n",
    "uniprot_data = uniprot_data.head(2000)\n",
    "uniprot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e3100f",
   "metadata": {},
   "source": [
    "## This cell performs label encoding for the EC numbers (Enzyme Commission numbers) in the UniProt dataset. The EC numbers are first extracted from the dataset and then converted into integer labels using LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3140d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "ec_numbers = uniprot_data['EC number']  # Extract the EC number column\n",
    "label_encoder = LabelEncoder()  # Initialize the label encoder\n",
    "ecnumber_labels = label_encoder.fit_transform(ec_numbers)  # Convert EC numbers to integer labels\n",
    "\n",
    "# Create mappings between labels and indices\n",
    "label_to_index = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))  # Mapping from label to index\n",
    "index_to_label = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))  # Mapping from index to label\n",
    "\n",
    "# Save the mappings to files\n",
    "with open('./label_to_index.pkl', 'wb') as f:\n",
    "    pickle.dump(label_to_index, f)  # Save the label-to-index mapping\n",
    "with open('./index_to_label.pkl', 'wb') as f:\n",
    "    pickle.dump(index_to_label, f)  # Save the index-to-label mapping\n",
    "\n",
    "# Generate an array of labels for further processing\n",
    "ecnumber_label = np.array([label_to_index[ec] for ec in ec_numbers])\n",
    "\n",
    "label_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad6e23561fe5ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.2 Protein Sequence Feature Extraction (Knowledge review)\n",
    "- **Definition**: Protein language models, such as ESM2 (Evolutionary Scale Modeling 2), are pre-trained deep learning models that can encode protein sequences into numerical representations. These models capture the contextual information and structural properties of amino acids in the sequence.\n",
    "- There are different variants of ESM2 with parameter counts ranging from 35M to 15B. Among them, the commonly used ESM2-T33-650M_UR50D model has about 650 million parameters.\n",
    "- ESM2 was pretrained on large-scale protein sequence databases, including UniRef50 (a non-redundant protein sequence database containing 50% sequence identity) and other publicly available protein sequence datasets.\n",
    "- ESM2 accepts protein sequences ESM2 accepts as input protein sequences, typically represented as one-dimensional sequences of amino acids, and the model generates embedding vectors for each position, which can be used for a variety of downstream tasks.\n",
    "- **Example**:\n",
    "\n",
    "   - if the download is slow, you can download the model from https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt and put it in /data/home/{yourusername}/.cache/torch/hub/checkpoints/ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3c00c",
   "metadata": {},
   "source": [
    "## This cell defines a function `batch_esm_embedding`, which is used to extract ESM model embeddings for a given set of sequences. \n",
    "## The function processes sequences in batches, feeding them into the ESM2 model, and generates sequence-level embeddings which are stored in dictionary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199192e38a6b8062",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T05:56:06.052903700Z",
     "start_time": "2024-11-12T05:55:54.884408700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function for batch extraction of ESM embeddings\n",
    "def batch_esm_embedding(sequences, batch_size=4):\n",
    "    \"\"\"\n",
    "    Extracts ESM embeddings for a list of protein sequences in batches.\n",
    "\n",
    "    Main steps:\n",
    "    1. Load the pre-trained ESM2 model and its alphabet.\n",
    "    2. Process sequences in batches to save GPU memory.\n",
    "    3. Generate ESM representations (average embedding vector from layer 6) for each batch.\n",
    "    4. Return a mapping of sequences to their corresponding embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - sequences (list of str): List of input protein sequences.\n",
    "    - batch_size (int, optional): The size of each batch, default is 128.\n",
    "\n",
    "    Returns:\n",
    "    - embeddings_dict (dict): A dictionary mapping each sequence to its corresponding embedding vector.\n",
    "    \"\"\"\n",
    "    # Check if GPU is available, prefer using GPU if possible\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the pre-trained ESM2 model and its alphabet\n",
    "    model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "    model = model.to(device).eval()  # Switch the model to evaluation mode\n",
    "    batch_converter = alphabet.get_batch_converter()  # Get the batch converter\n",
    "    \n",
    "    # Initialize a dictionary to store the embeddings\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    # Process the sequences in batches\n",
    "    for i in tqdm(range(0, len(sequences), batch_size), desc=\"Processing batches\"):\n",
    "        # Get the current batch of sequences\n",
    "        batch_seqs = sequences[i:i + batch_size]\n",
    "        \n",
    "        # Convert the sequences into the format required by the ESM model\n",
    "        data = [(idx, seq) for idx, seq in enumerate(batch_seqs)]\n",
    "        _, _, batch_tokens = batch_converter(data)\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "        # Disable gradient calculation and get the embeddings\n",
    "        with torch.no_grad():\n",
    "            # Use the model to calculate the representations from layer 6 (embedding for each sequence)\n",
    "            results = model(batch_tokens, repr_layers=[6])\n",
    "            sequence_embeddings = results[\"representations\"][6].mean(dim=1).cpu().numpy()\n",
    "\n",
    "        # Update the dictionary with the sequences and their embeddings\n",
    "        embeddings_dict.update({seq: emb for seq, emb in zip(batch_seqs, sequence_embeddings)})\n",
    "\n",
    "    # Return the embeddings dictionary\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be6ed1e",
   "metadata": {},
   "source": [
    "## This cell use the batch_esm_embedding function to extract embeddings for protein sequences and save the embeddings dictionary.\n",
    "## Convert the extracted embeddings into NumPy arrays and prepare the input data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cae613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the list of protein sequences from the data\n",
    "sequence = uniprot_data['Sequence'].to_list()  # Extract all protein sequences and store them in a list\n",
    "\n",
    "# Generate sequence embeddings using the ESM model\n",
    "esm_embeddings_dict = batch_esm_embedding(sequence)\n",
    "\n",
    "# Save the embeddings dictionary to a file\n",
    "with open('./esm_embeddings_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(esm_embeddings_dict, f)  # Serialize the embeddings dictionary to a local file using pickle\n",
    "\n",
    "# Extract embeddings from the dictionary and convert them to a NumPy array\n",
    "esm_embeddings = np.array([esm_embeddings_dict[seq] for seq in sequence])\n",
    "# Record the dimensions of the embeddings for use in later model building\n",
    "seq_shape = esm_embeddings.shape[1]\n",
    "\n",
    "# Inspect the embedding results\n",
    "esm_embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e85de8",
   "metadata": {},
   "source": [
    "## 2.Machine Learning Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb410c",
   "metadata": {},
   "source": [
    "## This cell mainly performs the following tasks: \n",
    "- Ensure that the lengths of labels and features are consistent to maintain the integrity of the training data. \n",
    "- Randomly split the embedding data, labels, and sequences into training and test sets.\n",
    "- Train a classification model using ExtraTreesClassifier and make predictions on the test set.\n",
    "- Compare the predicted results with the actual results to calculate and output the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43591ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the lengths of the labels and embedding features match to avoid mismatched data errors\n",
    "assert len(ecnumber_label) == len(esm_embeddings), \"The lengths of labels and features do not match\"\n",
    "\n",
    "# Randomly split the data into training and test sets (80% training, 20% test)\n",
    "Train_data, Test_data, Train_label, Test_label, Train_seq, Test_seq = train_test_split(\n",
    "    esm_embeddings,  # Input feature embeddings\n",
    "    ecnumber_label,  # Labels\n",
    "    sequence,        # Sequences (optional, for further analysis)\n",
    "    test_size=0.2,   # Test set ratio\n",
    "    random_state=42  # Fixed random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Initialize the ExtraTrees classifier (a variant of random forests)\n",
    "model = ExtraTreesClassifier()  # Automatically selects default parameters\n",
    "model.fit(Train_data, Train_label)  # Train the model using the training data\n",
    "\n",
    "# Output the shape of the data and labels to check if they match expectations\n",
    "print(\"Training data feature shape:\", Train_data.shape)\n",
    "print(\"Training data label shape:\", Train_label.shape)\n",
    "print(\"Test data feature shape:\", Test_data.shape)\n",
    "print(\"Test data label shape:\", Test_label.shape)\n",
    "\n",
    "# Use the trained model to predict on the test set\n",
    "Test_label_pred = model.predict(Test_data)\n",
    "\n",
    "# Compare the predicted results with the actual labels and display the first 20 samples (predicted vs actual)\n",
    "print(\"First 20 predicted labels (raw values):\", Test_label_pred[:20])\n",
    "print(\"First 20 actual labels (raw values):\", Test_label[:20])\n",
    "\n",
    "# Calculate and output the model's accuracy\n",
    "accuracy = np.mean(Test_label_pred == Test_label)  # Calculate the proportion of correct predictions\n",
    "print(\"Model accuracy:\", accuracy)  # Output the final accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4591ad",
   "metadata": {},
   "source": [
    "### Question: Map the first label in the predicted labels back to the corresponding EC number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d896ac0",
   "metadata": {},
   "source": [
    "## This cell mainly performs the following tasks: \n",
    "- Sequence similarity calculation: Implements the function calculate_identity, which calculates the similarity (match score) between two sequences based on global sequence alignment.\n",
    "- Maximum similarity between test and training sets: For each sequence in the test set, calculates the maximum similarity with all sequences in the training set.\n",
    "- Parallel acceleration: Uses multiprocessing.Pool and tqdm to calculate the maximum similarity for each sequence in the test set in parallel, and displays a progress bar.\n",
    "- Results output: Prints the highest similarity (as a percentage) between each test sequence and sequences in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the similarity (match percentage) between two sequences\n",
    "def calculate_identity(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Calculate the match percentage (similarity) between two sequences.\n",
    "    Uses the global sequence alignment algorithm globalxx (match +1, mismatch 0, no other penalties).\n",
    "    \"\"\"\n",
    "    # Perform global sequence alignment and return all possible alignments\n",
    "    alignments = pairwise2.align.globalxx(seq1, seq2)\n",
    "\n",
    "    # Retrieve the best alignment result (the highest score alignment)\n",
    "    best_alignment = alignments[0]\n",
    "\n",
    "    # Extract the number of matches and the total alignment length from the alignment result\n",
    "    matches = best_alignment[2]  # index 2 is the number of matches\n",
    "    total = best_alignment[4]    # index 4 is the total alignment length\n",
    "\n",
    "    # Calculate the match percentage (the proportion of matched characters)\n",
    "    identity_percent = (matches / total) * 100\n",
    "\n",
    "    # Return the match percentage\n",
    "    return identity_percent\n",
    "\n",
    "# Example: Compare the match percentage of two sequences\n",
    "seq1 = 'MTEITAAMVKELRESTGAGMMDCKNALSETQHEWFAAKRQGKLSPWITGRKTGQDEHILLMNDGWQ'\n",
    "seq2 = 'MTEITAAMVKELRESTGAGMMDCKNALSETQHEWFAALLMNDGWQ'\n",
    "\n",
    "# Calculate the match percentage for the example sequences\n",
    "identity = calculate_identity(seq1, seq2)\n",
    "\n",
    "# Print the match percentage\n",
    "identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the maximum match percentage between a test sequence and all training sequences\n",
    "def calculate_max_identity_for_test(test_seq, train_seqs):\n",
    "    \"\"\"\n",
    "    For a given test sequence, calculate its match percentage with all sequences in the training set and return the highest match percentage.\n",
    "    \"\"\"\n",
    "    # Iterate over all training sequences, calculate the match percentage, and return the maximum value\n",
    "    similarities = [calculate_identity(test_seq, train_seq) for train_seq in train_seqs]\n",
    "    return max(similarities)\n",
    "\n",
    "# Use partial to fix the training set parameter and generate a new function for parallel computation\n",
    "calculate_max_identity_fn = partial(calculate_max_identity_for_test, train_seqs=Train_seq)\n",
    "\n",
    "# Use a multiprocessing pool to calculate the maximum match percentage for each test sequence in parallel\n",
    "with multiprocessing.Pool(processes=60) as pool:\n",
    "    # Use tqdm to show the progress bar and parallelize the calculation\n",
    "    results = list(tqdm(pool.map(calculate_max_identity_fn, Test_seq), total=len(Test_seq)))\n",
    "\n",
    "# Print the maximum match percentage for each test sequence\n",
    "for i, test_seq in enumerate(Test_seq):\n",
    "    print(test_seq)\n",
    "    print(f\"Test sequence {i + 1}: Max identity = {results[i]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe57501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the length and content of the highest similarity results between the test set sequences and the training set\n",
    "print('Test sequences maximum similarity with training set:', len(results), results)\n",
    "\n",
    "# Print the length and content of the true labels for the test set\n",
    "print('True labels:', len(Test_label), Test_label)\n",
    "\n",
    "# Print the length and content of the predicted labels for the test set\n",
    "print('Predicted labels:', len(Test_label_pred), Test_label_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea89eb0",
   "metadata": {},
   "source": [
    "## This cell visualizes the prediction accuracy and the data count for each similarity interval using a dual-axis plot.\n",
    "- The cell computes the accuracy of predictions and the count of samples within specific similarity intervals, then plots these values on a dual Y-axis chart.\n",
    "- The left Y-axis shows the data count for each interval, while the right Y-axis displays the corresponding prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0966f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the plotting and numerical calculation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming results, y_true_test, y_pred_test are the data you provided\n",
    "# Creating the list of intervals (0-10, 10-20, ..., 90-100)\n",
    "intervals = np.arange(0, 101, 10)  # Creating similarity intervals from 0 to 100 with step size of 10\n",
    "accuracy_per_interval = []  # List to store prediction accuracy for each interval\n",
    "count_per_interval = []  # List to store the number of samples for each interval\n",
    "\n",
    "# Looping through each interval\n",
    "for i in range(len(intervals) - 1):\n",
    "    # Getting the lower and upper bounds of the current interval\n",
    "    lower_bound = intervals[i]  # Lower bound of the interval\n",
    "    upper_bound = intervals[i + 1]  # Upper bound of the interval\n",
    "    \n",
    "    # Finding indices of samples within the current interval\n",
    "    in_range_indices = np.where((np.array(results) >= lower_bound) & (np.array(results) < upper_bound))[0]\n",
    "    \n",
    "    # If there are samples in the current interval, calculate prediction accuracy\n",
    "    if len(in_range_indices) > 0:\n",
    "        # Getting the true labels and predicted labels for the current interval\n",
    "        y_true_in_range = np.array(Test_label)[in_range_indices]\n",
    "        y_pred_in_range = np.array(Test_label_pred)[in_range_indices]\n",
    "        \n",
    "        # Calculating accuracy\n",
    "        accuracy = np.sum(y_true_in_range == y_pred_in_range) / len(y_true_in_range)\n",
    "        accuracy_per_interval.append(accuracy)  # Storing the accuracy\n",
    "        \n",
    "        # Recording the number of samples in the current interval\n",
    "        count_per_interval.append(len(in_range_indices))\n",
    "    else:\n",
    "        # If no data in the current interval, set accuracy to 0\n",
    "        accuracy_per_interval.append(0)\n",
    "        count_per_interval.append(0)\n",
    "\n",
    "# Creating a dual-axis plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))  # Creating a figure and axes\n",
    "\n",
    "# Left Y-axis: Plotting the data count per interval (bar chart)\n",
    "ax1.bar(intervals[:-1], count_per_interval, width=8, color='lightblue', label='Data Count', align='edge')\n",
    "ax1.set_xlabel('Identity (%)')  # Setting X-axis label\n",
    "ax1.set_ylabel('Data Count', color='b')  # Setting left Y-axis label\n",
    "ax1.tick_params(axis='y', labelcolor='b')  # Setting color of left Y-axis tick labels\n",
    "\n",
    "# Right Y-axis: Plotting prediction accuracy (line chart)\n",
    "ax2 = ax1.twinx()  # Creating right Y-axis sharing the X-axis\n",
    "ax2.plot(intervals[:-1], accuracy_per_interval, marker='o', linestyle='-', color='r', label='Accuracy')  # Plotting line chart\n",
    "ax2.set_ylabel('Accuracy', color='r')  # Setting right Y-axis label\n",
    "ax2.tick_params(axis='y', labelcolor='r')  # Setting color of right Y-axis tick labels\n",
    "\n",
    "# Adding title and grid\n",
    "plt.title('Prediction Accuracy and Data Count per Similarity Interval')  # Setting the title\n",
    "ax1.grid(True)  # Enabling grid\n",
    "\n",
    "# Displaying the legend\n",
    "fig.tight_layout()  # Adjusting layout to prevent label overlap\n",
    "plt.show()  # Displaying the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4264274",
   "metadata": {},
   "source": [
    "## Question: What conclusions can you draw from this figure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12b7e1",
   "metadata": {},
   "source": [
    "## 3.Deep Learning Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd216f5",
   "metadata": {},
   "source": [
    "## This code handles the preparation of data for training and testing a model, including splitting datasets, saving sequence data, and setting up data loaders for training and validation.\n",
    "- Data Conversion: Converts esm_embeddings and ecnumber_label into PyTorch tensors.\n",
    "- Dataset Splitting: Splits the data into training and testing sets (80/20 split).\n",
    "- Sequence Saving: Saves the training and testing sequences into pickle files.\n",
    "- Dataset Creation: Creates training and validation datasets from the training data.\n",
    "- Data Loaders: Sets up data loaders for the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert esm_embeddings and ecnumber_label to PyTorch tensors\n",
    "esm_embeddings = torch.tensor(esm_embeddings, dtype=torch.float32)\n",
    "ecnumber_label = torch.tensor(ecnumber_label, dtype=torch.long)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "Train_data, Test_data, Train_label, Test_label, Train_seq, Test_seq = train_test_split(\n",
    "    esm_embeddings.numpy(), ecnumber_label.numpy(), sequence, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert the numpy arrays back to PyTorch tensors\n",
    "Train_data = torch.tensor(Train_data, dtype=torch.float32)\n",
    "Train_label = torch.tensor(Train_label, dtype=torch.long)\n",
    "Test_data = torch.tensor(Test_data, dtype=torch.float32)\n",
    "Test_label = torch.tensor(Test_label, dtype=torch.long)\n",
    "\n",
    "# Save the training and testing sequences into separate pickle files\n",
    "with open('./train_sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(Train_seq, f)\n",
    "with open('./test_sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(Test_seq, f)\n",
    "\n",
    "# Create a dataset from the training data and labels\n",
    "full_train_dataset = TensorDataset(Train_data, Train_label)\n",
    "\n",
    "# Split the dataset into training and validation sets (90% training, 10% validation)\n",
    "train_size = int(0.9 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Define batch size for loading data\n",
    "batch_size = 128\n",
    "\n",
    "# Create data loaders for training and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a998b",
   "metadata": {},
   "source": [
    "## This cell defines a neural network model, DeepEC, which is designed for predicting EC numbers from protein sequences. It uses a fully connected architecture with hidden layers, activation functions, batch normalization, and dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb48ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class DeepEC(nn.Module):\n",
    "    def __init__(self, seq_shape, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_dims = [256, 128, 64]\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(seq_shape, self.hidden_dims[0]),\n",
    "            nn.ReLU(), nn.BatchNorm1d(self.hidden_dims[0]),\n",
    "            nn.Linear(self.hidden_dims[0], self.hidden_dims[1]),\n",
    "            nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_dims[1], self.hidden_dims[2])\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_dims[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7053617",
   "metadata": {},
   "source": [
    "## This cell sets up the training loop for the DeepEC model, which is trained on a dataset to predict EC numbers. It includes model initialization, loss calculation, optimization, and evaluation metrics (accuracy, training and validation loss) over multiple epochs.\n",
    "- Model Setup: Defines the device for training (GPU if available), initializes the model, loss function (CrossEntropyLoss), optimizer (Adam), and learning rate scheduler.\n",
    "- Training Loop: Runs for 200 epochs, performing forward passes, loss calculation, backpropagation, and optimization.\n",
    "- Evaluation: After each epoch, calculates training loss, validation loss, and accuracy, updating the learning rate if necessary based on validation loss.\n",
    "- Plotting: Visualizes the training/validation loss and accuracy across epochs for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeepEC(seq_shape=seq_shape, num_classes=len(label_to_index)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "train_losses, val_losses, accuracy_values = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = []\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    epoch_val_loss, y_true, y_pred = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for val_data, val_labels in val_loader:\n",
    "            val_data, val_labels = val_data.to(device), val_labels.to(device)\n",
    "            outputs = model(val_data)\n",
    "            val_loss = criterion(outputs, val_labels)\n",
    "            epoch_val_loss.append(val_loss.item())\n",
    "            y_true.append(val_labels)\n",
    "            y_pred.append(outputs.argmax(dim=1))\n",
    "\n",
    "    avg_train_loss = np.mean(epoch_train_loss)\n",
    "    avg_val_loss = np.mean(epoch_val_loss)\n",
    "    accuracy_value = (torch.cat(y_true) == torch.cat(y_pred)).float().mean().item()\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    accuracy_values.append(accuracy_value)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy_value:.4f}\")\n",
    "\n",
    "# Plotting and evaluation\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over Epochs')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracy_values, label='Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b589a",
   "metadata": {},
   "source": [
    "## This code evaluates the performance of the trained model on the test set by predicting class labels and calculating the accuracy.\n",
    "- Evaluation Mode: Switches the model to evaluation mode to disable dropout and batch normalization updates.\n",
    "- Inference: Iterates over the test set in batches, performs forward passes to obtain predictions, and stores the true and predicted labels.\n",
    "- Accuracy Calculation: Compares the predicted labels with the true labels and calculates the test accuracy.\n",
    "- Output: Prints the calculated test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb13b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predicting results for the test set\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists for storing true labels and predicted labels\n",
    "y_true_test, y_pred_test = [], []\n",
    "\n",
    "# Disable gradient calculation during inference\n",
    "with torch.no_grad():\n",
    "    # Iterate over the test set\n",
    "    for test_data, test_label in DataLoader(TensorDataset(Test_data, Test_label), batch_size=batch_size):\n",
    "        test_data, test_label = test_data.to(device), test_label.to(device)\n",
    "        \n",
    "        # Model prediction\n",
    "        outputs = model(test_data)\n",
    "        \n",
    "        # Get the predicted class labels\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        \n",
    "        # Append true and predicted labels to the lists\n",
    "        y_true_test.append(test_label)\n",
    "        y_pred_test.append(preds)\n",
    "\n",
    "# Convert the lists to tensors\n",
    "y_true_test = torch.cat(y_true_test)\n",
    "y_pred_test = torch.cat(y_pred_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = (y_true_test == y_pred_test).float().mean().item()\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec435e8e",
   "metadata": {},
   "source": [
    "### Question: What factors are limiting the accuracy here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c0896",
   "metadata": {},
   "source": [
    "### This cell calculates the highest similarity between each sequence in the test set and the sequences in the training set. The function is defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ee14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the partial function to fix the train_seqs parameter\n",
    "calculate_max_identity_fn = partial(calculate_max_identity_for_test, train_seqs=Train_seq)\n",
    "\n",
    "# Use a multiprocessing pool to parallelize the computation and display progress\n",
    "with multiprocessing.Pool(processes=60) as pool:\n",
    "    results = list(tqdm(pool.map(calculate_max_identity_fn, Test_seq), total=len(Test_seq)))\n",
    "\n",
    "# Print the results for each test sequence\n",
    "for i, test_seq in enumerate(Test_seq):\n",
    "    print(test_seq)\n",
    "    print(f\"Test sequence {i + 1}: Max identity = {results[i]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the length and contents of the 'results' list\n",
    "print(len(results), results)\n",
    "\n",
    "# Print the length and contents of the 'y_true_test' list (true labels)\n",
    "print(len(y_true_test), y_true_test)\n",
    "\n",
    "# Print the length and contents of the 'y_pred_test' list (predicted labels)\n",
    "print(len(y_pred_test), y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This cell visualizes the prediction accuracy and the data count for each similarity interval using a dual-axis plot.\n",
    "- The cell computes the accuracy of predictions and the count of samples within specific similarity intervals, then plots these values on a dual Y-axis chart.\n",
    "- The left Y-axis shows the data count for each interval, while the right Y-axis displays the corresponding prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "intervals = np.arange(0, 101, 10)\n",
    "accuracy_per_interval = []\n",
    "count_per_interval = []\n",
    "\n",
    "# Iterate over each interval\n",
    "for i in range(len(intervals) - 1):\n",
    "    # Get the current interval range\n",
    "    lower_bound = intervals[i]\n",
    "    upper_bound = intervals[i + 1]\n",
    "    \n",
    "    # Find the indices within the current interval\n",
    "    in_range_indices = np.where((np.array(results) >= lower_bound) & (np.array(results) < upper_bound))[0]\n",
    "    \n",
    "    # If there is data in the current interval, calculate the prediction accuracy\n",
    "    if len(in_range_indices) > 0:\n",
    "\n",
    "        # Get the true and predicted labels for the current interval\n",
    "        y_true_in_range = np.array(y_true_test.cpu())[in_range_indices]\n",
    "        y_pred_in_range = np.array(y_pred_test.cpu())[in_range_indices]\n",
    "        \n",
    "        # Calculate the accuracy\n",
    "        accuracy = np.sum(y_true_in_range == y_pred_in_range) / len(y_true_in_range)\n",
    "        accuracy_per_interval.append(accuracy)\n",
    "        \n",
    "        # Record the sample count for the current interval\n",
    "        count_per_interval.append(len(in_range_indices))\n",
    "    else:\n",
    "        # If there is no data in the current interval, set accuracy to NaN or 0\n",
    "        accuracy_per_interval.append(0)\n",
    "        count_per_interval.append(0)\n",
    "\n",
    "# Create a dual Y-axis plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Left Y-axis: plot the data count for each interval (bar chart)\n",
    "ax1.bar(intervals[:-1], count_per_interval, width=8, color='lightblue', label='Data Count', align='edge')\n",
    "ax1.set_xlabel('Identity (%)')\n",
    "ax1.set_ylabel('Data Count', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "# Right Y-axis: plot the prediction accuracy (line plot)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(intervals[:-1], accuracy_per_interval, marker='o', linestyle='-', color='r', label='Accuracy')\n",
    "ax2.set_ylabel('Accuracy', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "# Add title and grid\n",
    "plt.title('Prediction Accuracy and Data Count per Similarity Interval')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Display the legend\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6866b1d7",
   "metadata": {},
   "source": [
    "## 4.Prediction Using Published Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ffa214",
   "metadata": {},
   "source": [
    "## 4.1 CLEAN\n",
    "- You can directly use ./CLEAN/results/ecoli_genome_clean_result.json, which contains the results of EC number prediction on the Escherichia coli genome generated by the CLEAN model.\n",
    "- If you want to perform EC number prediction using the CLEAN model, you can go to ./CLEAN/BIO.00.CLEAN_genome.ipynb\n",
    "\n",
    "## 4.2 DeepECtransformer\n",
    "- ./ecoli_genome_deepprozyme_result is the result file generated by the pre-trained DeepECtransformer model for EC number prediction on the Escherichia coli genome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce1a41",
   "metadata": {},
   "source": [
    "## 5.Result Analysis and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af986098",
   "metadata": {},
   "source": [
    "## The goal of this cell is to compare the prediction results of CLEAN and DeepECTransformer with the data from the UniProt database.\n",
    "- First, load the UniProt data and perform some cleaning tasks, such as filling missing values, extracting relevant columns, \n",
    "- and preprocessing EC numbers and gene names. Afterward, the code formats the data accordingly for further analysis or comparison tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f931813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Compare the prediction results of CLEAN and DeepECTransformer with the UniProt database\n",
    "# Input file paths\n",
    "uniprot_file = './uniprotkb_taxonomy_id_83333_2024_02_19.tsv'  # UniProt data file\n",
    "clean_result_file = './ecoli_genome_clean_result.json'  # CLEAN prediction results\n",
    "deepectransformer_result_file = './ecoli_genome_deepprozyme_result.json'  # DeepECTransformer prediction results\n",
    "gene2entry_mapping_file = './iML1515_gene2entry_mapping.json'  # Gene to UniProt entry mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the UniProt data file\n",
    "uniprot_data = pd.read_csv(uniprot_file, sep='\\t')  # Read UniProt file using tab as the delimiter\n",
    "uniprot_data.fillna('', inplace=True)  # Fill missing values with empty strings\n",
    "uniprot_data = uniprot_data[['Entry', 'Gene Names', 'EC number']]  # Extract necessary columns (UniProt entry, gene names, and EC number)\n",
    "\n",
    "# Process the Gene Names column by splitting gene names into lists (using space as the delimiter)\n",
    "uniprot_data['Gene Names'] = uniprot_data['Gene Names'].apply(lambda x: x.split(' '))\n",
    "\n",
    "# Process the EC number column by splitting EC numbers into lists (using '; ' as the delimiter) and removing empty values\n",
    "uniprot_data['EC number'] = uniprot_data['EC number'].apply(lambda x: x.split('; '))  # Split EC numbers by '; '\n",
    "uniprot_data['EC number'] = uniprot_data['EC number'].apply(lambda x: [ec for ec in x if ec != ''])  # Remove empty EC numbers\n",
    "\n",
    "# View the processed UniProt data\n",
    "uniprot_data  # Output the cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60591c",
   "metadata": {},
   "source": [
    "## The purpose of this code is to load the prediction result files from DeepECTransformer and CLEAN, and check the number of results contained in each file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c4644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DeepECTransformer prediction results and check the number of data entries\n",
    "with open(deepectransformer_result_file, 'r') as f:  # Open the DeepECTransformer result file\n",
    "    deepectransformer_result = json.load(f)  # Read and load the JSON file contents\n",
    "\n",
    "print(len(deepectransformer_result))  # Print the number of data entries in the DeepECTransformer result\n",
    "print(deepectransformer_result)  # Print the DeepECTransformer result\n",
    "\n",
    "# Load the CLEAN prediction results and check the number of data entries\n",
    "with open(clean_result_file, 'r') as f:  # Open the CLEAN result file\n",
    "    clean_result = json.load(f)  # Read and load the JSON file contents\n",
    "\n",
    "print(len(clean_result))  # Print the number of data entries in the CLEAN result\n",
    "print(clean_result)  # Print the CLEAN result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7bc8cf",
   "metadata": {},
   "source": [
    "### Question: What are the prediction results for b3045 using DeepECTransformer and CLEAN, respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d06fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve the EC number corresponding to a given UniProt entry\n",
    "def entry2ECnumber(entry, uniprot_data):\n",
    "    tmp = uniprot_data[uniprot_data['Entry'] == entry]  # Search for the matching entry in the UniProt data\n",
    "    return tmp['EC number'].to_list()[0]  # Return the EC number of the matching entry, converted to a list and get the first element\n",
    "\n",
    "# Define a function to retrieve the EC number corresponding to a given gene name\n",
    "def gene2ECnumber(gene, gene2entry_mapping, uniprot_data):\n",
    "    if gene in gene2entry_mapping:  # Check if the gene exists in the gene-to-entry mapping\n",
    "        entry = gene2entry_mapping[gene]  # Get the corresponding UniProt entry\n",
    "        ECnumber = entry2ECnumber(entry, uniprot_data)  # Use entry2ECnumber function to get the EC number\n",
    "        return ECnumber  # Return the EC number\n",
    "    else:\n",
    "        return '-'  # If no matching gene is found, return '-'\n",
    "\n",
    "# Read the gene-to-UniProt entry mapping dictionary from the JSON file\n",
    "with open(gene2entry_mapping_file, 'r') as f:  # Open the file to read its contents\n",
    "    gene2entry_mapping = json.load(f)  # Load the content of the file into a dictionary\n",
    "\n",
    "gene2entry_mapping  # Output the gene-to-UniProt entry mapping dictionary\n",
    "# Create a list of genes, containing all genes from the gene-to-UniProt entry mapping\n",
    "model_gene_lst = list(gene2entry_mapping.keys())\n",
    "print(model_gene_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca26d2b",
   "metadata": {},
   "source": [
    "## This code iterates through the gene list 'model_gene_lst' and collects relevant information for each gene, including the UniProt entry, EC number, and the prediction results from CLEAN and DeepECTransformer. \n",
    "- If any relevant information for a gene is missing, the missing value is represented by '-'. \n",
    "- Finally, all the collected data is stored in a dictionary called 'result', which contains the gene name, UniProt entry, UniProt EC number, and the prediction results from CLEAN and DeepECTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store gene information and related prediction results\n",
    "result = {\n",
    "    'gene': [],  # Store gene names\n",
    "    'UniProt_entry': [],  # Store corresponding UniProt entry for the gene\n",
    "    'UniProt_EC': [],  # Store the corresponding EC number for the gene\n",
    "    'CLEAN': [],  # Store CLEAN prediction results\n",
    "    'DeepECtransformer': []  # Store DeepECTransformer prediction results\n",
    "}\n",
    "\n",
    "# Use tqdm to display a progress bar while iterating through the gene list 'model_gene_lst'\n",
    "for i in tqdm(model_gene_lst):\n",
    "    result['gene'].append(i)  # Add the current gene name to the result dictionary\n",
    "\n",
    "    # Check if the gene exists in the gene-to-UniProt entry mapping\n",
    "    if i in gene2entry_mapping:\n",
    "        result['UniProt_entry'].append(gene2entry_mapping[i])  # Add the corresponding UniProt entry\n",
    "    else:\n",
    "        result['UniProt_entry'].append('-')  # If no entry is found, append a placeholder '-'\n",
    "\n",
    "    # Get the EC number for the gene (use placeholder if missing)\n",
    "    result['UniProt_EC'].append(gene2ECnumber(i, gene2entry_mapping, uniprot_data))\n",
    "\n",
    "    # Check if CLEAN prediction result is available, add prediction or placeholder\n",
    "    if i in clean_result:\n",
    "        result['CLEAN'].append(clean_result[i])  # Add CLEAN prediction result\n",
    "    else:\n",
    "        result['CLEAN'].append('-')  # If no result is available, append a placeholder\n",
    "\n",
    "    # Check if DeepECTransformer prediction result is available, add prediction or placeholder\n",
    "    if i in deepectransformer_result:\n",
    "        result['DeepECtransformer'].append(deepectransformer_result[i])  # Add DeepECTransformer prediction result\n",
    "    else:\n",
    "        result['DeepECtransformer'].append('-')  # If no result is available, append a placeholder\n",
    "\n",
    "# Convert the result dictionary to a DataFrame\n",
    "result_df = pd.DataFrame(result)\n",
    "\n",
    "# Filter out rows with no UniProt entry (those with a UniProt_entry value of '-')\n",
    "result_df = result_df[result_df['UniProt_entry'] != '-']\n",
    "\n",
    "result_df.head(5)  # Display the first 5 rows of the resulting DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257cf7f",
   "metadata": {},
   "source": [
    "## This code evaluates the performance of the CLEAN and DeepECTransformer models in predicting EC numbers for given protein sequences and compares their predictions with a database to assess their ability to distinguish enzymes from non-enzymes. It calculates and visualizes the True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN) for both models, along with the corresponding confusion matrices.\n",
    "\n",
    "- Calculates TP, TN, FP, FN for DeepECTransformer and plots the confusion matrix.\n",
    "- Calculates TP, TN, FP, FN for CLEAN and plots the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948178fe",
   "metadata": {},
   "source": [
    "## DeepECtransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c93d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate True Positive (TP): Correctly predicted positive cases\n",
    "TP = len(result_df[(result_df['UniProt_EC'].apply(len) > 0) & (result_df['DeepECtransformer'] != '-')])\n",
    "\n",
    "# Calculate False Positive (FP): Predicted as positive but actually negative\n",
    "FP = len(result_df[(result_df['UniProt_EC'].apply(len) == 0) & (result_df['DeepECtransformer'] != '-')])\n",
    "\n",
    "# Calculate True Negative (TN): Predicted as negative and actually negative\n",
    "TN = len(result_df[(result_df['UniProt_EC'].apply(len) == 0) & (result_df['DeepECtransformer'] == '-')])\n",
    "\n",
    "# Calculate False Negative (FN): Predicted as negative but actually positive\n",
    "FN = len(result_df[(result_df['UniProt_EC'].apply(len) > 0) & (result_df['DeepECtransformer'] == '-')])\n",
    "\n",
    "# Output the components of the confusion matrix\n",
    "print(TP)  # Output the number of True Positives\n",
    "print(FP)  # Output the number of False Positives\n",
    "print(TN)  # Output the number of True Negatives\n",
    "print(FN)  # Output the number of False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde15b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Create the confusion matrix\n",
    "confusion_matrix1 = np.array([[TP, FP], [FN, TN]])\n",
    "\n",
    "# Set figure size and resolution\n",
    "fig = plt.figure(figsize=(2, 2), dpi=300)\n",
    "\n",
    "# Define subplot layout\n",
    "gs = GridSpec(1, 2, width_ratios=[4, 0.1])\n",
    "\n",
    "# Plot the main confusion matrix\n",
    "ax1 = plt.subplot(gs[0])\n",
    "im1 = ax1.imshow(confusion_matrix1, cmap='PuBu', interpolation='nearest', vmin=1, vmax=1400, aspect='auto')\n",
    "\n",
    "# Hide the axes\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.grid(False)\n",
    "\n",
    "# Add text labels for matrix elements\n",
    "ax1.text(0, 0, f\"TP\\n{confusion_matrix1[0, 0]}\", ha='center', va='center', fontsize=6, color='black')\n",
    "ax1.text(1, 0, f\"FP\\n{confusion_matrix1[0, 1]}\", ha='center', va='center', fontsize=6, color='black')\n",
    "ax1.text(0, 1, f\"FN\\n{confusion_matrix1[1, 0]}\", ha='center', va='center', fontsize=6, color='black')\n",
    "ax1.text(1, 1, f\"TN\\n{confusion_matrix1[1, 1]}\", ha='center', va='center', fontsize=6, color='black')\n",
    "\n",
    "# Assuming Accuracy is defined, calculate the accuracy\n",
    "Accuracy = (TP + TN) / (TP + FP + FN + TN)  # Example calculation\n",
    "ax1.set_xlabel(f\"DeepECtransformer ACC = {Accuracy:.2f}\", fontsize=6, labelpad=1)\n",
    "\n",
    "# Add a border around the image\n",
    "for spine in ax1.spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    spine.set_linewidth(0.2)\n",
    "\n",
    "# Plot the color bar\n",
    "cbar_ax = plt.subplot(gs[1])\n",
    "cbar = fig.colorbar(im1, cax=cbar_ax)\n",
    "cbar.ax.tick_params(labelsize=6)\n",
    "\n",
    "# Set the color bar scale\n",
    "cbar.ax.yaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "\n",
    "# Add a border around the color bar\n",
    "for spine in cbar_ax.spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    spine.set_linewidth(0.2)\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.02)\n",
    "\n",
    "# Display the confusion matrix plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1667711",
   "metadata": {},
   "source": [
    "## CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc80f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate True Positive (TP): Correctly predicted positive cases\n",
    "TP = len(result_df[(result_df['UniProt_EC'].apply(len) > 0) & (result_df['CLEAN'] != '-')])\n",
    "\n",
    "# Calculate False Positive (FP): Predicted as positive but actually negative\n",
    "FP = len(result_df[(result_df['UniProt_EC'].apply(len) == 0) & (result_df['CLEAN'] != '-')])\n",
    "\n",
    "# Calculate True Negative (TN): Predicted as negative and actually negative\n",
    "TN = len(result_df[(result_df['UniProt_EC'].apply(len) == 0) & (result_df['CLEAN'] == '-')])\n",
    "\n",
    "# Calculate False Negative (FN): Predicted as negative but actually positive\n",
    "FN = len(result_df[(result_df['UniProt_EC'].apply(len) > 0) & (result_df['CLEAN'] == '-')])\n",
    "\n",
    "# Output the components of the confusion matrix\n",
    "print(TP)  # Output the number of True Positives\n",
    "print(FP)  # Output the number of False Positives\n",
    "print(TN)  # Output the number of True Negatives\n",
    "print(FN)  # Output the number of False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee19726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Create the confusion matrix\n",
    "confusion_matrix1 = np.array([[TP, FP], [FN, TN]])\n",
    "\n",
    "# Set figure size and resolution\n",
    "fig = plt.figure(figsize=(2, 2), dpi=300)\n",
    "\n",
    "# Define the subplot layout\n",
    "gs = GridSpec(1, 2, width_ratios=[4, 0.1])\n",
    "\n",
    "# Plot the main confusion matrix\n",
    "ax1 = plt.subplot(gs[0])\n",
    "im1 = ax1.imshow(confusion_matrix1, cmap='PuBu', interpolation='nearest', vmin=1, vmax=1400, aspect='auto')\n",
    "\n",
    "# Hide axes ticks\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.grid(False)\n",
    "\n",
    "# Add text labels for matrix elements\n",
    "ax1.text(0, 0, f\"TP\\n{confusion_matrix1[0, 0]}\", ha='center', va='center', fontsize=6, color='black')\n",
    "ax1.text(1, 0, f\"FP\\n{confusion_matrix1[0, 1]}\", ha='center', va='center', fontsize=6, color='black')\n",
    "ax1.text(0, 1, f\"FN\\n{confusion_matrix1[1, 0]}\", ha='center', va='center', fontsize=6, color='black')\n",
    "ax1.text(1, 1, f\"TN\\n{confusion_matrix1[1, 1]}\", ha='center', va='center', fontsize=6, color='black')\n",
    "\n",
    "# Calculate accuracy (assuming the variable `Accuracy` is defined)\n",
    "Accuracy = (TP + TN) / (TP + FP + FN + TN)  # Example calculation\n",
    "ax1.set_xlabel(f\"CLEAN ACC = {Accuracy:.2f}\", fontsize=6, labelpad=1)\n",
    "\n",
    "# Add borders to the image\n",
    "for spine in ax1.spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    spine.set_linewidth(0.2)\n",
    "\n",
    "# Plot the colorbar\n",
    "cbar_ax = plt.subplot(gs[1])\n",
    "cbar = fig.colorbar(im1, cax=cbar_ax)\n",
    "cbar.ax.tick_params(labelsize=6)\n",
    "\n",
    "# Set colorbar ticks\n",
    "cbar.ax.yaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "\n",
    "# Add borders to the colorbar\n",
    "for spine in cbar_ax.spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    spine.set_linewidth(0.2)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.02)\n",
    "\n",
    "# Display the confusion matrix plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f7874",
   "metadata": {},
   "source": [
    "### Question: The CLEAN method predicts an EC number for all protein sequences. Why is the False Negative (FN) in the confusion matrix for CLEAN equal to 1 instead of 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0843a7",
   "metadata": {},
   "source": [
    "### Question: The CLEAN method predicts an EC number for all protein sequences. What issues could this cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248bd4",
   "metadata": {},
   "source": [
    "## This code matches the prediction results with the actual data and evaluates their accuracy by comparing the predictions of CLEAN and DeepECTransformer row by row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba1fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate whether CLEAN prediction results match the actual EC numbers\n",
    "# For each row, check if any element in row['CLEAN'] exists in row['UniProt_EC']\n",
    "result_df['CLEAN_res'] = result_df.apply(\n",
    "    lambda row: any(x in row['UniProt_EC'] for x in row['CLEAN']), axis=1\n",
    ")\n",
    "\n",
    "# Calculate whether DeepECTransformer prediction results match the actual EC numbers\n",
    "# For each row, check if any element in row['DeepECTransformer'] exists in row['UniProt_EC']\n",
    "result_df['DeepECtransformer_res'] = result_df.apply(\n",
    "    lambda row: any(x in row['UniProt_EC'] for x in row['DeepECtransformer']), axis=1\n",
    ")\n",
    "\n",
    "# Calculate the matching ratio for CLEAN: the number of matching rows divided by the total number of rows\n",
    "print('CLEAN:', len(result_df[result_df['CLEAN_res'] == True]) / len(result_df))\n",
    "\n",
    "# Calculate the matching ratio for DeepECTransformer: the number of matching rows divided by the total number of rows\n",
    "print('DeepECtransformer:', len(result_df[result_df['DeepECtransformer_res'] == True]) / len(result_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdabe0d",
   "metadata": {},
   "source": [
    "## Question: If considering only the first digit, first two digits, or first three digits of the EC number,what are the prediction accuracies of CLEAN and DeepECTransformer respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b965eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Type your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
